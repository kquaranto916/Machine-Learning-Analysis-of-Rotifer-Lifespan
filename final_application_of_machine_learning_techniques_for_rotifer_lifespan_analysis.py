# -*- coding: utf-8 -*-
"""Final_Application_of_Machine_Learning_Techniques_for_Rotifer_Lifespan_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1szdy4kpmFA2iJ5Q7GuN6oFt3eZ8pgv6c

##**Project Title: Application of Machine Learning Techniques for Rotifer Lifespan Analysis**.

### Authors: Colton Pelletier, Victoria Freitas, Ella Costigan
### Advisors: Dr. Hum Nath Bhandari, Dr. Christopher Burtner

### Last Modified: 04/18/2023

### **Project Overview**

Aquatic rotifers are a relatively new model organism to the biology of aging. Rotifers possess many characteristics that make them suitable for aging studies, such as a short natural life span and parthenogenic reproduction. Brachionus plicatilis is a free-swimming rotifer that feeds upon single-celled algae in brackish water.  We also find that the B. plicatilis life span can be extended by rapamycin at concentrations as low as 5Î¼M.  With this encouraging data that B. plicatilis responds to pharmaceuticals we look to test with additional treatments.  With the work of Convolutional Neural Network (CNN) and machine learning we have designed a program to assay lifespan photographs for rotifer deaths and output that information to be usable to create Kaplan-Meyer curves.
"""

from google.colab import drive
drive.mount('/content/drive')

train_data_path = '/content/drive/MyDrive/MATH479_Group1/Data/train_data/'
test_data_path = '/content/drive/MyDrive/MATH479_Group1/Data/test_data/'
validation_data_path = '/content/drive/MyDrive/MATH479_Group1/Data/val_data/'

import os
import cv2
import pandas as pd
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

"""# **Reading Images**

## Step 1: Load Train images
"""

from numpy.lib.type_check import imag
X_train = []

for filename in os.listdir(train_data_path):
    img = cv2.imread(os.path.join(train_data_path, filename))
    #img = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY)
    #img = cv2.resize(img, (100,100), interpolation=cv2.COLOR_BGR2GRAY)
    img = cv2.resize(img, (64, 64))#
    #img = img /255
    X_train.append(img)

X_train = np.array(X_train)

"""**Possible Task**: We can save this numpy array for future use.

"""

print(X_train.shape)

output_dir_path = '/content/drive/MyDrive/Data_Science_Capstone_Project1/Results/'

print(X_train[0][0])

"""## Step 2: Load Test Images"""

X_test = []
for filename in os.listdir(test_data_path):
    img = cv2.imread(os.path.join(test_data_path, filename))
    #img = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY)
    #img = cv2.resize(img, (100,100), interpolation=cv2.COLOR_BGR2GRAY)
    img = cv2.resize(img, (64, 64))#
    #img = img /255
    X_test.append(img)
X_test = np.array(X_test)

print(X_test[0][0])

"""## Step 3:  Load Validation Images"""

X_val = []

for filename in os.listdir(validation_data_path):
  img = cv2.imread(os.path.join(validation_data_path, filename))
  #img = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY)
  #img = cv2.resize(img, (100,100), interpolation=cv2.COLOR_BGR2GRAY)
  img = cv2.resize(img, (64, 64))#
  #img = img /255
  X_val.append(img)

# convert it to a numpy array
X_val = np.array(X_val)

print(X_val[0][0])

"""### Possible Task:
 We can save this numpy array for future use.
"""

print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

"""# **Reading Labels Data**"""

train_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/FinalDataRotiferCountTrain.csv')
test_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/Test_DataRotiferCountTrain.csv')
val_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/Val_DataRotiferCountTrain.csv')

y_train =  train_data_csv.iloc[:, 1].values
y_test =  test_data_csv.iloc[:, 1].values
y_val =  val_data_csv.iloc[:, 1].values

print(y_train.shape)
print(y_test.shape)
print(y_val.shape)

print(y_train)
print(y_test)
print(y_val)

"""# **Input Visualization**

"""

def visuzlize_frames(frames_list, num_frames):
 fig = plt.figure(figsize = (12, 12))
 random_range = random.sample(range(len(frames_list)), num_frames)

 for counter, random_index in enumerate(random_range, 1):
   bgr_frame = frames_list[random_index]
   rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)
   plt.subplot(3, 4, counter);
   plt.imshow(rgb_frame);
   #plt.axis('off')
 fig.savefig(output_dir_path + "frames_visualizations.png",dpi=600)
 plt.show()

output_dir_path = '/content/drive/MyDrive/MATH479_Group1/Results/'

num_frames = 4

visuzlize_frames(X_train[100:104], num_frames)

print(X_train[100])

"""# **Iuput Preparation**

## Step 1: Set Parameters
"""

SEQUENCE_LENGTH = 1
IMAGE_HEIGHT = 64  #Try a couple of variations: 64, 100, 128, or actual height
IMAGE_WIDTH =  64  #Try a couple of variations: 64, 100, 128, or actual height
NUM_CHANNELS = 3
NUM_CLASSES = 6

"""## Step 2: Reshape Input Features"""

X_train = X_train.reshape(X_train.shape[0], SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
X_test =  X_test.reshape(X_test.shape[0], SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
X_val =  X_val.reshape(X_val.shape[0], SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

"""## Step 3: Normalize Data if Necessary

X_train = X_train / 255
X_test =  X_test / 255

X_val = X_val / 255

## Step 4: Reshape Target Output
"""

print("y_train", y_train)
print("y_val" , y_val)
print("y_test", y_test)

"""### Make output class matrix"""

from keras.utils import to_categorical

#Convert one-dimensional class arrays (output) into 6 -dimensional class matrices:

y_train  =  to_categorical(y_train, NUM_CLASSES)

y_test  =  to_categorical(y_test, NUM_CLASSES)

y_val   =   to_categorical(y_val, NUM_CLASSES)

#to_categorical: Converts a class vector (integers) to binary class matrix.
print(y_train.shape)
print(y_test.shape)
print(y_val.shape)

print(y_train[0])
print(y_test[0])
print(y_val[0])

"""#**Building Models**

## **Supprting Libraries**
"""

import tensorflow as tf
from keras.models import Sequential
from keras.utils import np_utils
from keras.layers import Dense, MaxPool2D, Conv2D
from keras.layers import Flatten
from keras.initializers import Constant
from tensorflow.keras import optimizers
from sklearn import metrics

"""## **Supporting Functions**"""

def write_dic_to_file(dic_name, file_name):
  file = open(file_name, 'w')
  file.write(str(dic_name))
  file.close()

import ast
def read_dic_from_file(file_name):
  file = open(file_name, "r")
  contents = file.read()
  dictionary = ast.literal_eval(contents)
  file.close()
  return dictionary



def create_error_plot(model_history):
  loss =  model_history.history['loss']
  val_loss = model_history.history['val_loss']
  accuracy = model_history.history['accuracy']
  val_acc = model_history.history['val_accuracy']

  #epochs = np.arange(len(loss))

  epochs = model_history.epoch
  fig = plt.figure(figsize = (12,5))

  plt.subplot(121)
  plt.plot(epochs, loss, color = 'red', marker = "s")
  plt.plot(epochs, val_loss, marker = "^")
  plt.legend(["loss", "validation loss"], loc="best")
  plt.xlabel("epoch")
  plt.ylabel("loss")
  plt.title("Training and Validation Loss")

  plt.subplot(122)
  plt.plot(epochs, accuracy, color = 'red', marker = "o")
  plt.plot(epochs, val_acc, marker = "v")
  plt.legend(["accuracy", "validation accuracy"], loc="best")
  plt.xlabel("epoch")
  plt.ylabel("accuracy")
  plt.title("Training and Validation Accuracy Scores")
  fig.savefig(output_dir_path + "error_plot.png", dpi = 600)
  plt.show()

"""## **Model 1: CNN Model**

##**Input Shape for CNN Model**
"""

X_train = X_train.reshape(X_train.shape[0], IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
X_test =  X_test.reshape(X_test.shape[0], IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
X_val =  X_val.reshape(X_val.shape[0], IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

"""## Step 1: Build CNN Model"""

def Build_CNN_Model(conv_layers, dense_layers, kernel_size,  input_shape, output_dim,  optimizer, learning_rate, verbose):

  #====Start model initialization====
  model = Sequential()

  #=====Add convolution layers=====
  for i in range(len(conv_layers)):
    if len(conv_layers) == 1:
      model.add(Conv2D(conv_layers[i], kernel_size = kernel_size, input_shape = input_shape, activation='relu',  strides = 1, padding = 'same'))
      model.add(MaxPool2D(padding = 'same'))
    else:

      if i == 0:
        model.add(Conv2D(conv_layers[i], kernel_size = kernel_size, input_shape = input_shape, activation='relu',  strides = 1, padding = 'same'))
        model.add(MaxPool2D(padding='same'))

      else:
        model.add(Conv2D(conv_layers[i], kernel_size = kernel_size, activation='relu',  strides = 1, padding = 'same'))
        model.add(MaxPool2D(padding='same'))

  #=====Flatten the output from Convolution + Pooling Layer =====
  model.add(Flatten())

  #=====Adding fully connected (dense ) layers =====
  for i in range(len(dense_layers)):
    model.add(Dense(dense_layers[i],  activation='relu'))
    model.add(Dropout(0.10))

  #=====Adding output layers =====
  model.add(Dense(output_dim,  activation = 'softmax'))

  #=====Select Optimizer =====
  if optimizer == 'Adam':
    opt = optimizers.Adam(learning_rate = learning_rate)
  elif optimizer == 'Adagrad':
    opt = optimizers.Adagrad(learning_rate = learning_rate)
  elif optimizer == 'Nadam':
    opt = optimizers.Nadam(learning_rate = learning_rate)
  elif optimizer == 'Adadelta':
    opt = optimizers.Adadelta(learning_rate= learning_rate)
  elif optimizer == 'RMSprop':
    opt = optimizers.RMSprop(learning_rate= learning_rate)
  else:
    print("No optimizer found in the list(['Adam', 'Adagrad','Nadam', 'Adadelta', 'RMSprop'])! \
    Please apply your optimizer manually...")

  #=====Compile Model =====
  model.compile(loss='categorical_crossentropy', optimizer = opt,  metrics = ['accuracy'])

  if verbose == 1:
    print(model.summary())
  return model

"""### Testing Building Process of CNN Model"""

Build_CNN_Model(conv_layers =  [4, 8],
                dense_layers =  [10, 5],
                kernel_size = (2,2),
                input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                output_dim = NUM_CLASSES,
                optimizer= 'Adam',
                learning_rate= 0.001,
                verbose = 1)

"""## Step 3: Implementing CNN Model"""

def CNN_Model_Implementation(conv_layers =  [8,16],
               dense_layers = [4],
               kernel_size = (2,2),
               input_shape = (64, 64, 3),
               output_dim  =  6,
               optimizer = 'Adam',
               learning_rate = 0.01,
               X_train = None,
               y_train = None,
               X_test =  None,
               y_test =  None,
               X_val = None,
               y_val = None,
               batch_size = 2,
               epochs = 3,
               verbose = 0):

  print('Building CNN Model.......................\n')
  #Build CNN model
  model = Build_CNN_Model(conv_layers, dense_layers,  kernel_size, input_shape, output_dim, optimizer, learning_rate, verbose =0 )

  print('Fitting  CNN Model.......................\n')
  #Fit CNN model
  early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 10)
  #can remove callbacks if issue with epochs
  history = model.fit(X_train, y_train, batch_size = batch_size, epochs= epochs, validation_data = (X_val, y_val), callbacks = [early_stopping_callback], verbose = verbose)

  print('Calculating predictions and scores.......................\n')


  #Calculate predictions
  pred = model.predict(X_test)

  #Calculate test scores
  scores = model.evaluate(X_test, y_test)

  #Create plots
  #create_error_plot(history)

  print('Collecting outputs.......................\n')
  output = {
            'model': model,
            'model_history': history,
            'test_loss': scores[0],
            'test_acc': scores[1],
            'test_predictions': pred,
       }

  print('Congratuations! All the processes are completed successfully....\n')
  return output

"""## **CNN Case I**
- Conv_layer = [8, 16]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- batch_size = 1
- epochs = 20
- Total numbers of trainable parameters = 41,668



"""

case1_cnn_model_output = CNN_Model_Implementation(conv_layers =  [8, 16],
                    dense_layers = [10,5],
                    kernel_size = (2,2) ,
                    input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case1_cnn_model_output['model_history'])
print("Test_Accuracy\n",case1_cnn_model_output['test_acc'] )
print("Test_Loss\n", case1_cnn_model_output['test_loss'] )
print("Test Prediction\n", case1_cnn_model_output['test_predictions'])

"""## **CNN Case 2**
- Conv_layer = [8, 16]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 4**
- epochs = 20
- Total numbers of trainable parameters = 43,541
"""

case2_cnn_model_output = CNN_Model_Implementation(conv_layers =  [8, 16],
                    dense_layers = [10, 5],
                    kernel_size = (2,2) ,
                    input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 4,
                    epochs = 20,
                    verbose = 1
                    )


create_error_plot(case2_cnn_model_output['model_history'])
print("Test_Accuracy\n",case2_cnn_model_output['test_acc'] )
print("Test_Loss\n", case2_cnn_model_output['test_loss'] )
print("Test Prediction\n", case2_cnn_model_output['test_predictions'])

"""## **CNN Case 3**

- Conv_layer = [64, 32, 16,8,4]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 1**
- epochs = 20
- Total numbers of trainable parameters = 12,033
"""

case3_cnn_model_output = CNN_Model_Implementation(conv_layers =  [64, 32, 16, 8, 4],
                    dense_layers = [10, 5],
                    kernel_size = (2,2) ,
                    input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )
create_error_plot(case3_cnn_model_output['model_history'])
print("Test_Accuracy\n",case3_cnn_model_output['test_acc'] )
print("Test_Loss\n", case3_cnn_model_output['test_loss'] )
print("Test Prediction\n", case3_cnn_model_output['test_predictions'])

"""## **CNN Case 4**

- Conv_layer = [128]
- Dense_layer = [20]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 1**
- epochs = 20
- Total numbers of trainable parameters =  2,623,250
"""

Build_CNN_Model(conv_layers =  [128],
                dense_layers =  [20],
                kernel_size = (2,2),
                input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                output_dim = NUM_CLASSES,
                optimizer= 'Adam',
                learning_rate= 0.001,
                verbose = 1)

case4_cnn_model_output = CNN_Model_Implementation(conv_layers =  [128],
                    dense_layers = [20],
                    kernel_size = (2,2) ,
                    input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case4_cnn_model_output['model_history'])
print("Test_Accuracy\n",case4_cnn_model_output['test_acc'] )
print("Test_Loss\n", case4_cnn_model_output['test_loss'] )
print("Test Prediction\n", case4_cnn_model_output['test_predictions'])

"""## **CNN Case 5**
- Normalized Data
- Conv_layer = [8,16]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 1**
- epochs = 20
- Total numbers of trainable parameters =  41,693
"""

Build_CNN_Model(conv_layers =  [8,16],
                dense_layers =  [10,5],
                kernel_size = (2,2),
                input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                output_dim = NUM_CLASSES,
                optimizer= 'Adam',
                learning_rate= 0.001,
                verbose = 1)

case5_cnn_model_output = CNN_Model_Implementation(conv_layers =  [8,16],
                    dense_layers = [10,5],
                    kernel_size = (2,2) ,
                    input_shape =  (IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case5_cnn_model_output['model_history'])
print("Test_Accuracy\n",case5_cnn_model_output['test_acc'] )
print("Test_Loss\n", case5_cnn_model_output['test_loss'] )
print("Test Prediction\n", case5_cnn_model_output['test_predictions'])

"""# **CNN Cases Results**:

**Case 1**:
Test_Accuracy
 0.27473583817481995

**Case 2**:
Test_Accuracy
 0.34966379404067993

**Case 3**:
Test_Accuracy
 0.3410182595252991

**Case 4**:
Test_Accuracy
 0.4063400626182556

**Case 5**:
Test_Accuracy
 0.32372719049453735

### Case 4 has the highest test accuracy out of all the cases with hyperparameters:

- Conv_layer = [128]
- Dense_layer = [20]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 1**
- epochs = 20
- Total numbers of trainable parameters =  2,623,250

## **Model 2: CONV_LSTM_Model**

### **Input Shape for CONV_LSTM Model**
"""

X_train = X_train.reshape(X_train.shape[0], SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
X_test =  X_test.reshape(X_test.shape[0], SEQUENCE_LENGTH,IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
X_val =  X_val.reshape(X_val.shape[0], SEQUENCE_LENGTH,IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

"""## **Build Conv_Lstm Model**"""

def Build_Conv_Lstm_Model(conv_layers, dense_layers, input_shape,  output_dim,  optimizer = 'Adam', learning_rate = 0.001, verbose = 1):

    #Step 1: Initialize Model========#
    model = Sequential()

    #Step 2: Add Convolution Hidden layers=====#
    for i in range(len(conv_layers)):
      if len(conv_layers) == 1:
        #========Add first Hidden Layer===========#
        model.add(ConvLSTM2D(filters = conv_layers[i], kernel_size = (2, 2), recurrent_dropout = 0.05, return_sequences = True, input_shape = input_shape))
        model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))
        model.add(TimeDistributed(Dropout(0.05)))

      else:
        if i == 0:
          #========Add first Hidden Layer===========#
          model.add(ConvLSTM2D(filters = conv_layers[i], kernel_size = (2, 2), recurrent_dropout = 0.10, return_sequences = True, input_shape = input_shape))
          model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))
          model.add(TimeDistributed(Dropout(0.05)))

        else:
          model.add(ConvLSTM2D(filters = conv_layers[i] , kernel_size = (2, 2), recurrent_dropout=0.10, return_sequences=True))
          model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))
          model.add(TimeDistributed(Dropout(0.05)))

    #Step 3: Flatten Layer ===========#
    model.add(Flatten())

    #Step 4: Add fully connected (dense) layers =====#
    for i in range(len(dense_layers)):
      model.add(Dense(dense_layers[i],  activation='relu'))
      model.add(Dropout(0.10))

    #Step 5: Add Output Layer =========#
    model.add(Dense(output_dim,  activation = 'softmax'))

    #Step 6: Select Optimizer and Learning Rate =========#
    if optimizer == 'Adam':
      opt = optimizers.Adam(learning_rate = learning_rate)
    elif optimizer == 'Adagrad':
      opt = optimizers.Adagrad(learning_rate = learning_rate)
    elif optimizer == 'Nadam':
      opt = optimizers.Nadam(learning_rate = learning_rate)
    elif optimizer == 'Adadelta':
      opt = optimizers.Adadelta(learning_rate= learning_rate)
    elif optimizer == 'RMSprop':
      opt = optimizers.RMSprop(learning_rate= learning_rate)
    else:
      print("No optimizer found in the list(['Adam', 'Adagrad','Nadam', 'Adadelta', 'RMSprop'])! \n Please apply your optimizer manually...")

    #Step 7: Compile Model =========#
    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ["accuracy"])

    #Step 7: Display Model summary and model Plots
    if verbose == 1:
      print(model.summary())
      plot_model(model, to_file = output_dir_path+'conv-lstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
    # Return the constructed convlstm model.

    #Step 8: return the model
    return model

"""## Testing Model Building"""

conv_layers = [8, 16]

dense_layers = [4]

input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS) #data_format = "channels_last"

output_dim = NUM_CLASSES

optimizer = 'Adam'

learning_rate = 0.001

verbose = 1

Build_Conv_Lstm_Model(conv_layers, dense_layers, input_shape,  output_dim,  optimizer = 'Adam', learning_rate = 0.001, verbose = 1)

"""# **Implementing Conv-Lstm-Model**"""

def Conv_Lstm_Model_Implementation(conv_layers =  [8,16],
               dense_layers = [10,5],
               kernel_size = (2,2),
               input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
               output_dim  =  6,
               optimizer = 'Adam',
               learning_rate = 0.01,
               X_train = None,
               y_train = None,
               X_test =  None,
               y_test =  None,
               X_val = None,
               y_val = None,
               batch_size = 1,
               epochs = 20,
               verbose = 0):

  print('Building Conv_Lstm Model.......................\n')
  #Build CNN model
  model = Build_Conv_Lstm_Model(conv_layers, dense_layers, input_shape,  output_dim,  optimizer, learning_rate, verbose)

  print('Fitting  Conv_Lstm Model ......................\n')
  #Fit Conv_Lstm Model
  early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 5)
  #can remove callbacks if issue with epochs
  history = model.fit(X_train, y_train, batch_size = batch_size, epochs= epochs, validation_data = (X_val, y_val), callbacks = [early_stopping_callback], verbose = verbose)

  print('Calculating predictions and scores.......................\n')


  #Calculate predictions
  pred = model.predict(X_test)

  #Calculate test scores
  scores = model.evaluate(X_test, y_test)

  #Create plots
  #create_error_plot(history)

  print('Collecting outputs.......................\n')
  output = {
            'model': model,
            'model_history': history,
            'test_loss': scores[0],
            'test_acc': scores[1],
            'test_predictions': pred,
       }

  print('Congratuations! All the processes are completed successfully....\n')
  return output

"""## **Conv_Lstm Case I**
- Conv_layer = [8, 16]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- batch_size = 1
- sequence_length = 1
- epochs = 20
- Total numbers of trainable parameters = 48,709



"""

case1_conv_lstm_output = Conv_Lstm_Model_Implementation(conv_layers =  [8,16],
                    dense_layers = [10,5],
                    kernel_size = (2,2) ,
                    input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case1_conv_lstm_output['model_history'])
print("Test_Accuracy\n",case1_conv_lstm_output['test_acc'] )
print("Test_Loss\n", case1_conv_lstm_output['test_loss'] )
print("Test Prediction\n",case1_conv_lstm_output['test_predictions'])

"""## **Conv_Lstm Case 2**
- Conv_layer = [8, 16]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 4**
- sequence_length = 1
- epochs = 20
- Total numbers of trainable parameters = 48,709

"""

case2_conv_lstm_output = Conv_Lstm_Model_Implementation(conv_layers =  [8,16],
                    dense_layers = [10,5],
                    kernel_size = (2,2) ,
                    input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 4,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case2_conv_lstm_output['model_history'])
print("Test_Accuracy\n",case2_conv_lstm_output['test_acc'] )
print("Test_Loss\n", case2_conv_lstm_output['test_loss'] )
print("Test Prediction\n",case2_conv_lstm_output['test_predictions'])

"""## **Conv_Lstm Case 3**

- Conv_layer = [64, 32, 16,8,4]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- sequence_length = 1
- **batch_size = 1**
- epochs = 20
- Total numbers of trainable parameters =134,645
"""

case3_conv_lstm_output = Conv_Lstm_Model_Implementation(conv_layers =  [64,32, 16, 8,4],
                    dense_layers = [10,5],
                    kernel_size = (2,2) ,
                    input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case3_conv_lstm_output['model_history'])
print("Test_Accuracy\n",case3_conv_lstm_output['test_acc'] )
print("Test_Loss\n", case3_conv_lstm_output['test_loss'] )
print("Test Prediction\n",case3_conv_lstm_output['test_predictions'])

"""## **Conv_Lstm Case 4**


- Conv_layer = [128]
- Dense_layer = [20]
- optimizer = 'Adam'
- learning rate = 0.001
- sequence_length = 1
- **batch_size = 1**
- epochs = 20
- Total numbers of trainable parameters =  2,623,250


"""

case4_conv_lstm_output = Conv_Lstm_Model_Implementation(conv_layers =  [128],
                    dense_layers = [20],
                    kernel_size = (2,2) ,
                    input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS),
                    output_dim  = NUM_CLASSES,
                    optimizer = 'Adam',
                    learning_rate = 0.001,
                    X_train = X_train,
                    y_train = y_train,
                    X_test = X_test, # we need to fix this issue
                    y_test =  y_test,
                    X_val = X_val,
                    y_val = y_val,
                    batch_size = 1,
                    epochs = 20,
                    verbose = 1
                    )

create_error_plot(case4_conv_lstm_output['model_history'])
print("Test_Accuracy\n",case4_conv_lstm_output['test_acc'] )
print("Test_Loss\n", case4_conv_lstm_output['test_loss'] )
print("Test Prediction\n",case4_conv_lstm_output['test_predictions'])

"""# **Conv_Lstm Results**
**Case 1**:
Test_Accuracy
 0.3093179762363434

**Case 2** :
Test_Accuracy
 0.4082612991333008

**Case 3** :
Test_Accuracy
 0.4063400626182556

**Case 4** :
Test_Accuracy
 0.3640730082988739

### Case 2 performed the best with case 3 at a close second

## **Conv_Lstm Case 2**
- Conv_layer = [8, 16]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- **batch_size = 4**
- sequence_length = 1
- epochs = 20
- Total numbers of trainable parameters = 48,709


## **Conv_Lstm Case 3**


- Conv_layer = [64, 32, 16,8,4]
- Dense_layer = [10,5]
- optimizer = 'Adam'
- learning rate = 0.001
- sequence_length = 1
- **batch_size = 1**

## **Visualization of Rotifer Lifespan**

## **TRUE Rotifer Lifespan from  Train Data**

---
"""

train_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/FinalDataRotiferCountTrain.csv')

time_steps = train_data_csv.index

y_train = train_data_csv.iloc[:, 1].values


plt.figure(figsize = (10,5))

plt.plot(time_steps, y_train, 'b' )
plt.xlabel("Time Steps")
plt.ylabel("Rotifer Counts")
plt.title("Life Span of Rotifer")
plt.show()

"""## **TRUE Rotifer Lifespan from Test Data**"""

test_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/Test_DataRotiferCountTrain.csv')

time_steps = test_data_csv.index

y_test = test_data_csv.iloc[:, 1].values


plt.figure(figsize = (10,5))

plt.plot(time_steps, y_test, 'b' )
plt.xlabel("Time Steps")
plt.ylabel("Rotifer Counts")
plt.title("Life Span of Rotifer")
plt.show()

"""## **TRUE Rotifer Lifespan from  Validation Data**"""

val_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/Val_DataRotiferCountTrain.csv')
time_steps = val_data_csv.index

y_val = val_data_csv.iloc[:, 1].values


plt.figure(figsize = (10,5))

plt.plot(time_steps, y_val, 'b' )
plt.xlabel("Time Steps")
plt.ylabel("Rotifer Counts")
plt.title("Life Span of Rotifer")
plt.show()

test_data_csv = pd.read_csv('/content/drive/MyDrive/MATH479_Group1/Data/FinalDataRotiferCountTest.csv')

# load the predicted values
predicted_values = np.load('/path/to/predicted_values.npy')

# extract the time steps and actual rotifer counts from the test data
time_steps = test_data_csv.index
y_test = test_data_csv.iloc[:, 1].values

# plot the predicted values and the actual values
plt.figure(figsize=(10, 5))
plt.plot(time_steps, predicted_values, 'r', label='Predicted Values')
plt.plot(time_steps, y_test, 'b', label='Actual Values')
plt.xlabel('Time Steps')
plt.ylabel('Rotifer Counts')
plt.title('Predicted vs. Actual Rotifer Counts')
plt.legend()
plt.show()

"""## **Things to Do**

- Analyzie thess results carefully.
- Maximum accuracy is 40%(not satisfactory results so far)
- Try various things to increase the accuracy at least 60%
- Models are tring to learn the data but we might have some issues with input output data creation.
- We need to make the  graphs of predicted life span of Rotifers. For this, we need to convet predictioin matrix to one dimentional list or array and then make plots like the above graphs.

- Use CLR and Transfer Learning to improve performance.

- Try to train models with additional data from more videos


"""